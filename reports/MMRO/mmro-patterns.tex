\documentclass[twoside]{article}
\usepackage{mmro15}
%\NOREVIEWERNOTES

\begin{document}
\newcommand{\evt}{ {\bf e} }
\newcommand{\pat}{ {\bf P} }
\title
    {Вероятностный подход к поиску поведенческих паттернов}
\author
    [Этот необязательный аргумент задаёт краткий список авторов; он нужен только в~том случае, если полный список не влезает в~колонтитул]
    {Вишневский~В.\,В., Ветров~Д.\,П.}
    %[Этот необязательный аргумент задаёт список авторов, выводимый на печать;
    %он нужен только в~том случае, когда есть отличия от основного списка, например,
    %когда надо пронумеровать авторов и соответствующие организации: Автор~И.\,О.$^1$]
\thanks
    {Работа выполнена при финансовой поддержке РФФИ(проект №08-01-00405),
гранта Президента РФ(МК3827.2010.9),
и федеральной целевой программы <<Научные и научно-педагогические кадры инновационной России>>
на 2009--2013 годы(контракт №П1265).

Авторы статьи выражают благодарность членам <<Лаборатории Нейробиологии Памяти>> при
Институте Нормальной Физиологии П.К.~Анохина, возглавляемой членом-корреспондентом РАН К.В.~Анохиным.
Отдельно благодарим Ирину Зарайскую, предоставившую нам экспериментальные данные по поведению.}
\email
    {valera.vishnevskiy@yandex.ru, vetrovd@yandex.ru}
\organization
    {Москва, Москвоский Государственный Университет им. М.\,В. Ломносова}
\abstract
    {В данной работе предложен новый метод для поиска скрытых закономерностей
(паттернов) в последовательностях событий, основанный на вероятностном представлении 
P"=Паттернов(probabilistic pattern) в дискретных последовательностях событий. 
Поиск производится снизу вверх: сначала находятся простые закономерности, потом, путем 
их соединения, образуются более сложные паттерны.  
Рассматривается применение данного алгоритма для анализа поведения мышей. 
Найденные паттерны используются для классификации животных. 
Проведено сравнение реализованного алгоритма с существующими аналогами, показавшее, 
что предложенный метод более устойчив к шуму в исходных данных.
}
\maketitle

Задача поиска закономерностей(стереотипов, паттернов, шаблонов~--- здесь синонимы) 
в поведении животных и людей крайне важна в современной нейробиологии и когнитивных науках. 
Выделив характерные паттерны, можно, например, делать выводы о сложности поведения различных 
особей, определять изменения в поведении наблюдаемых процессов, другими словами, решив задачу 
поиска паттернов, мы можем определенным образом {\em измерять} поведение особи, или группы особей.

В данной работе мы будем рассматривать временн\'{ы}е паттерны в <<структурном>>, или
<<эффектном>> описании~\cite[с.~57]{Martin_Bateson}. 
Исходными данными будет размеченное поведение особи, то есть последовательность пар 
<<момент времени>>, <<поведенческий акт>>. 
Неформально можно сказать, что интересующие нас паттерны "--- это упорядоченная 
последовательность поведенческих актов, следующие один за другим через
\emph{относительно инвариантные} временн\'{ы}е интервалы. 
Причем, этот паттерн должен повторяться в исходных данных \emph{достаточно} часто.

Несмотря на то, что описанные выше паттерны широко распространены в описании поведения,
стандартные статистические методы не подходят для их поиска: эти методы либо не учитывают 
всю сложность паттернов(например, периодические орбиты~\cite{Stoop}), либо 
оперируют такими понятиями как циклы, волны, тренды, что невозможно
напрямую использовать для поиска интересующих нас паттернов.

На сегодняшний день, для анализа таких поведенческих закономерностей
наиболее широкое распространение получил метод поиска T"=Паттернов(temporal patterns), предложенный в 2000-ом году
Магнусом Магнуссоном в~\cite{Magnusson}. 
% Наш метод, в основном, базируется на концепции
% Магнуссоновского подхода к поиску паттернов, с учетом основных недостатков метода поиска Т"=Паттернов,
% а именно, его сильной чувствительности к шуму в исходных данных.

\section{Основные определения. Понятие T"=Паттерна}
Пусть время наблюдения разбито на $N_t$ интервалов. 
В каждый момент {\itshape периода наблюдения} $[\,1,N_t]$ может произойти 
некоторое событие $\evt$({\itshape действие, поведенческий акт, event}) 
\footnote{ Чаще всего понимается, что в этот момент времени имеет место {\itshape начало} действия} 
из множества допустимых событий $\mathcal{E}$. 
Соответственно, каждому типу события сопоставляется множество моментов 
времени $TS(\evt)$, когда это событие имело место:
\[
\begin{aligned}
&TS(\evt) = \{ t^{\evt}_1,\dots, t^{\evt}_{N_{\evt} } \}, \\ 
&\evt\in\mathcal{E},\qquad
0\leqslant t^{\evt}_i \leqslant N_t,~~(i=1,\dots,N_t),
\end{aligned}
\]
здесь $N_{\evt}$~--- количество появлений события $\evt$ в данных.
% \begin{figure}[t]
% \label{CI}
% \noindent\centering{\includegraphics[width=\linewidth]{TPTSn.eps}}
% \caption{ Отношение критического интервала $[\,d_L, d_R]$ между событиями $A$ и $B$. }
% \end{figure}

\paragraph{Понятие Т"=Паттерна} 
включает в себя определение модели связи последовательных событий и способа 
их появления в данных(т.е. определения, что данная структура не случайна, 
а является \emph{закономерностью}). 
Каждое событие паттерна определяется фиксированным временн\'{ы}м интервалом, 
в течение которого это событие должно присутствовать после предыдущего события. 
Другими словами, расстояния между событиями моделируются равномерным распределением. 
% Т-Паттерн состоящий из $N_{\pat}$ событий обычно обозначается следующим образом:
% \[
% \begin{aligned}
%   \pat=\, & \evt_1[\,d^1_L, d^1_R]\,\evt_2\,[\,d^2_L, d^2_R]\,\dots \\
%         & \evt_{N_{\pat}-1}\,[\,d^{N_{\pat}-1}_L, d^{N_{\pat}-1}_R]\,\evt_{N_{\pat}}.
% \end{aligned}
% \]
% Запись $\evt_A[\,d_L, d_R]\evt_B$ обозначает, что событие $\evt_B$ должно присутствовать 
% во временн\'{о}м интервале $[\,d_L, d_R]$ после события $\evt_A$, чаще, чем это ожидается при предположении 
% о независимости событий(см.~рис.\,\ref{CI}). 
% Более точно, при помощи биномиальной схемы, считается вероятность текущей конфигурации распределения событий, 
% принимая гипотезу, о том, что вероятность встретить событие $\evt$ в какой-либо момент времени равна 
% $\frac{N_{\evt}}{N_t}$. 
% Если эта вероятность меньше заданного порога $\alpha$(обычно берется меньше $0{,}05$), 
% то гипотеза о независимом равномерном распределении событий отвергается и мы говорим, что 
% $\evt_A[\,d_L, d_R]\evt_B$ "--- Т"=Паттерн, соединенный отношением критической 
% связи(critical relation). 
% Точно такие же рассуждения можно провести, взяв вместо событий $\evt_A$ и $\evt_B$ составные Т"=Паттерны 
% $\pat_A$ и $\pat_B$. 
% Отличие заключается только в том, что нужно считать вхождения \emph{начала} Т"=Паттерна $\pat_B$ в 
% интервале $[\,d_L, d_R]$ после \emph{конца} Т"=Паттерна $\pat_A$. 

% \paragraph{Алгоритм поиска Т"=Паттернов.}
% Опишем этап \emph{конструирования} Т"=Паттернов: для всех пар Т"=Паттернов, определяем, 
% существует ли интервал $[\,d_L, d_R]$, связывающий эту пару паттернов критическим отношением. 
% Если такой интервал существует, то мы добавляем новый Т"=Паттерн. Если таких интервалов несколько,
% то предлагается взять самый длинный интервал.
% 
% Для очистки множества найденных паттернов от паттернов-дубликатов и неполных копий существующих паттернов,
% вводится процедура \emph{редукции} множества Т"=Паттернов: Т"=Паттерн $\pat_L$ считается менее полным, чем $\pat_R$, 
% если $\pat_L$ и $\pat_R$ появляются одинаково часто, и все события
% возникающие в $\pat_L$, также возникают в $\pat_R$. Все Т"=Паттерны попарно проверяются на полноту,
% и если один паттерн признан менее полным, то он удаляется из текущего множества Т"=Паттернов.
% 
% Предложенный алгоритм поиска заключается в итеративном повторении процедуры конструирования и процедуры редукции 
% Т"=Паттернов, пока текущее множество Т"=Паттернов не перестанет изменяться. 

\paragraph{Основной недостатк метода поиска Т"=Паттернов}
заключается, во-первых, в том, что само определение Т"=Паттерна не позволяет ему иметь 
пропуски событий. По этой причине метод становится крайне чувствителен к шуму в исходных данных, из-за чего
можно пропустить информативные длинные и сложные паттерны. Во-вторых, полученные Т"=Паттерны сильно специфичны
особи, в поведении которой они были найдены.

\section{Вероятностная модель паттерна. P"=Паттерны}

\begin{Def}
    \emph{Нечетким паттерном}, или P-Паттерном  $\pat$ длины $N_{\pat}$ назовем упорядоченную последовательность событий $\evt_i, (i=1,\dots,N_{\pat})$, 
где каждое событие паттерна характеризуется смещением и разбросом от предыдущего события. Будем записывать паттерн
$\pat$ в следующем виде:
$$ \pat=[\,\mu_1,\sigma_1]\evt_1[\,\mu_2,\sigma_2]\evt_2\dots[\,\mu_{N_{\pat}},\sigma_{N_{\pat}}]\evt_{N_{\pat}}, \quad \mu_1=0. $$
Здесь $\mu_i$ и $\sigma_i$ "--- метематическое ожидание и корень из дисперсии нормального распределения, моделирующего
величину времени, прошедшего между событиями.   
\end{Def}
\begin{figure}[t]
\label{ppttern}
\noindent\centering{\includegraphics[width=\linewidth]{il1.eps}}
\caption{ Вхождение нечеткого паттерна $\pat=\evt_A[\,\mu_A,\sigma_A]\evt_B[\,\mu_B,\sigma_B]\evt_C[\,\mu_C,\sigma_C]$.
в данные. Маркеры"=кружки соответствуют ожидаемым позициям событиям, закрашенные маркеры"=квадраты соответствуют 
позиции реальной позиции соответствующего события в данных.}
\end{figure}

Представление P"=Паттерна иллюстрировано на рис.\,1. 
% Если нам не важны параметры смещения и разброса событий, то P"=Паттерн можно записать мледующим образом: 
% $$\pat=\evt_1\evt_2\dots\evt_{N_{\pat}},$$
% или, если нам важна иерархия паттерна, то так: 
% $$\pat=((\evt_1\evt_2)(\evt_3\evt_4)).$$

Далее, чтобы иметь возможность обрабатывать пропуски в P"=Паттернах, 
введем понятие {\em функции потерь}, которая определяет <<штраф>> за пропуск
$m$ событий в паттерне длины $N_{\pat}$ следующим образом:
$$
f_{LOSS}(m,N)= \begin{cases}
   \exp\left(-\frac{\lambda m}{N_{\pat}}\right), & m < N, \\
   0,                                    & m=N.
   \end{cases}
$$
Здесь $\lambda$ является структурным параметром, определяющим уровень
<<нечеткости>> паттернов. Если этот параметр велик, то мы, по сути, запрещаем реализациям
паттерна иметь пропуски. Если выставить этот параметр слишком малым, то будут обнаруживаться
паттерны, не разу полностью не встречающиеся в данных, то есть закономерности могут быть  найдены
даже в случайных данных. 
% Данный параметр должен выставляться вручную, исходя из априорной информации
% о типе данных, уровне шума, и сложности поведения. 

% Теперь мы можем определить ключевое понятие для представленного метода поиска
% P"=Паттернов.
\begin{Def}
\emph{Правдоподобие паттерна} $\pat$~--- это функция, определенная
в каждый момент времени наблюдения $\varepsilon \: (\varepsilon=1,\dots, N_t)$ следующим образом:
\begin{multline}\label{LHOOD}
L_{\pat}(\varepsilon)=
f_{LOSS}(N_-,N_{\pat})
\prod_{i=1}^{N_{\pat}} 
  \left(    \frac1{ \sqrt{2\pi}\,\sigma_i }    \right)           \times \\ \times
\prod_{i\in \mathcal{N}_+}
  \exp\left( - \frac{ \delta_i^2 }{ 2\sigma_i^2 } \right),
\end{multline}
где $\delta_i$~--- расстояния от ожидаемой позиции события в P"=Паттерне до 
ближайшего события в данных(более наглядно см. рис.\,1). Т.е.:
$$
\delta_i=\min_{x\,\in\, T\!S(\evt_i)} \left|  
 \underbrace{\varepsilon+ \sum_{j=1}^{i-1}(\mu_j+\delta_j) + \mu_i}_{\text{ожидаемая позиция события}} -\, x
\right|,
$$
здесь, если событие было пропущено, то соответствующее $\delta_i=0$.
Далее, $N_-$~--- количество пропущенных событий в паттерне, а $\mathcal{N}_+$~--- множество индексов присутствующих в паттерне событий. Событие
считается пропущенным, если 
$\exp\left(- \frac{\delta_i^2}{2\sigma_i^2}\right)<\exp\left(-\frac{\lambda}{N_{\pat}}\right)$
% , т.е.соответствующее значение $\delta_i$ больше определенного предела
.
\end{Def}
По сути, правдоподобие показывает
насколько можно убыть уверенным, что данный P"=Паттерн начинается в определенный момент времени~$\varepsilon$.  

Заметим, что правдоподобие P"=Паттерна может быть отсчитано с конца, или с $m$-го события паттерна.
% Заметим, что правдоподобие P"=Паттерна можно считать не только начиная с первого события, но и, например, с конца P"=Паттерна. Для упрощения вычислений,
% значение правдоподобия паттерна $\pat$ с события $m$ можно рассчитывать по следующей формуле:
% $$
% L_{P,m}(\varepsilon)=L_P\left(\varepsilon+\sum_{j=1}^m\mu_j\right).
% $$ 

\begin{State}
    Математическое ожидание функции правдоподобия~\eqref{LHOOD} в момент времени~$\varepsilon$, 
    при условии, что в данный момент времени имеет место начало \emph{модельного} P"=Паттерна~$\pat$
    вычисляется следующим образом:

\[
\Expect\left[L_{\pat}(\varepsilon)\right] 
=\frac1{(2\sqrt\pi\,)^{N_{\pat}}\,\sigma_1\dots\sigma_{N_{\pat}}}.
% \Expect\left[L_{\pat}(\varepsilon)\right] = 
% \int\limits_{-\infty}^{+\infty}\!\dots\!\int\limits_{-\infty}^{+\infty}
% L_{\pat}(\varepsilon)  \times \\ \times
% \prod_{i=1}^{N_{\pat}}\frac1{\sqrt{2\pi}\,\sigma_i}\exp\left(-\frac{\delta_i}{2\sigma_i^2}\right)
% \, d\delta_1\dots d\delta_{N_{\pat}} =\\
% =\frac1{(2\pi)^{N_{\pat}}\prod_{i=1}^{N_{\pat}}\sigma_i^2}\: 
% \prod_{i=1}^{N_{\pat}} \; \int\limits_{-\infty}^{+\infty}\exp\left( -\frac{\delta_i^2}{\sigma_i^2} \right)\,d\delta_i =\\
% =\frac1{(2\sqrt\pi\,)^{N_{\pat}}\,\sigma_1\dots\sigma_{N_{\pat}}}.
\]
\end{State}
Доказательство приведено в~\cite{Diplom}.

Здесь используется тот факт, что межточечные расстояния 
между событиями в модельном P"=Паттерне распределены по нормальному 
закону:
$$
  \delta_i\sim\Normal(\,0,\sigma_i),\quad(i=1,\dots,N_{\pat}).
$$ 

% Теперь, вычислив функцию правдоподобия P"=Паттерна в данных, 
% мы можем отобрать только те ее максимумы, которые имеют один порядок
% с математическим ожиданием правдоподобия модельного P"=Паттерна, то есть
% считать, что P"=Паттерн $\pat$ встречается в моменты времени
Теперь мы можем считать, что P"=Паттерн $\pat$ имеет место быть только 
в следующие моменты времени:
\begin{equation}
\label{LH_maxs}
t \colon L_{\pat}(t) \geqslant \gamma\Expect\left[L_{\pat}(\varepsilon)\right],
\end{equation}
где $\gamma$ "--- заданная константа.

\paragraph{Конструирование P"=Паттернов.}
Рассмотрим пару P-Паттернов $\pat_L$(левый) и $\pat_R$(правый). Пусть $\{\alpha_i\}_{i=1,\dots,N_L}$ и $\{\beta_j\}_{j=1,\dots,N_R}$
 "--- значения правдоподобия соответствующих P"=Паттернов в моменты времени, соответствующие паттерны имели вхождения~\eqref{LH_maxs}. 
Важно, что правдоподобие левого P"=Паттерна отсчитывается с конца, так как мы ищем связь между концом левого паттерна и началом правого.
Также пусть,  $\{t_{L,i}\}_{i=1,\dots,N_L}$ и $\{t_{R,j}\}_{j=1,\dots,N_R}$~--- моменты времени, когда эти P"=Паттерны имели место
в смысле~\eqref{LH_maxs}. 
$N_L$ и $N_R$~--- количество вхождений паттернов $\pat_L$ и $\pat_R$, соответственно. Определим множество межточечных
расстояний:
$$
\rho=\{ t_{R,j}-t_{L,i}\: |\: t_{R,j} \geqslant t_{L,i}\}.
$$
Для каждого расстояния из этого множества введем соответствующий вес 
$
w_l = \ln (1+\alpha_i\beta_j), \quad l=1,\dots,M,
$
где $M=|\rho|.$

Рассмотрим гипотезу $H_0$, что моменты времени и веса вхождения P"=Паттернов распределены независимо и 
равномерно на всем наблюдаемом промежутке.
% : 
% $$
% t_{L,i}\sim\mathcal{U}[0,N_t], \quad t_{R,j}\sim\mathcal{U}[0,N_t].
% $$ 
Тогда плотность распределение введенных выше межточечных расстояний $\{t\}$ имеет следующий вид~\cite{Diplom}:
\begin{multline}
p_{LR}(t) =
\begin{cases}
  (N_t-t)\,\frac2{N_t^2} , & t \in [0,N_t], \\
  0, & x \not\in [0,N_t].
\end{cases} 
\end{multline}

Введем статистическую модель связи между паттернами(проверяемые параметры связи $\mu$ и $\sigma$ фиксированы):
$$
g_{\mu,\sigma}(t_i)=\frac{1}{\sqrt{2\pi}\,\sigma}\exp\left(- \frac{(t_i-\mu)^2}{2\sigma^2} \right).
$$
Рассмотрим следующую сумму:
\begin{equation}\label{k_stat}
k=\sum_{i=1}^M w_ig_{\mu,\sigma}(t_i), 
\end{equation}
где $t_i\sim p_{LR}$. 
% 
% Для оценки распределения $k$ проведем требуемые вычисления:
% 
% \begin{multline}
% \Expect \left[ g_{\mu,\sigma} \right] = 
% \int\limits_{0}^{N_t} g_{\mu,\sigma}(t)\,p_{LR}(t)\,dt \approx \\ \approx
% \frac2{N_t} \left(1-\frac{\mu}{N_t}\right),
% \end{multline}
% 
% \begin{multline}
% \Expect \left[ g^2_{\mu,\sigma} \right] = 
% \int\limits_{0}^{N_t}g^2_{\mu,\sigma}(t)\,p_{LR}(x)\,dt \approx \\ \approx
% \frac1{N_t\sqrt{\pi}\,\sigma} \left(1-\frac{\mu}{N_t}\right),
% \end{multline}
% 
% \begin{multline}
% \Var \left[ g_{\mu,\sigma} \right] = 
% \Expect \left[ g^2_{\mu,\sigma} \right]  - \left( \Expect \left[ g_{\mu,\sigma} \right] \right)^ 2 \approx \\ \approx
% \left( 1-\frac{\mu}{N_t} \right)\,\left( \frac1{{N_t}\sqrt{\pi}\,\sigma} - \frac{\mu}{{N_t}^2} \left( 1- \frac{\mu}{N_t} \right) \right).
% \end{multline}
% О распределении значений $w_i$ мы не можем делать никаких предположений, 
% поэтому в вычислениях будут подразумеваться их выборочное математическое ожидание и дисперсия:
% \begin{multline}
% \Expect \left[ \xi_i \right] = \Expect\left[ w g_{\mu,\sigma}(t) \right] = 
% \Expect\left[ w_i\right]    \Expect\left[  g_{\mu,\sigma}(t_i) \right] \approx \\ \approx
% \Expect[w] \: \frac2{N_t} \left(1-\frac{\mu}{N_t}\right).
% \end{multline}
% Запишем  дисперсию произведения независимых случайных величин:
% \begin{multline}
% \Var \left[ \xi_i \right] = \Var \left[ w\, g_{\mu,\sigma} \right] = 
% \left( \Expect\left[ g_{\mu,\sigma} \right]\right)^2 \Var[w] +
% \Var\left[ g_{\mu,\sigma} \right] (\Expect[w])^2 +\\+ \Var[w]\, \Var\left[ g_{\mu,\sigma} \right].
% \end{multline}
% Тогда по Центральной Предельной Теореме:


\begin{Theorem}
    При выполнении $H_0$ статистика $k$ распределена по нормальному закону:
    \begin{equation}\label{k_norm}
	\begin{aligned}
	      &k\: \sim\: \mathcal{N}\left(\mu_\ast, \sigma_\ast^2 \right), \:\text{где} \\
	      &\mu_\ast\approx M\,  \Expect[w] \: \frac2{N_t} \left(1-\frac{\mu}{N_t}\right) \\
	      &\sigma_\ast^2 \approx  \frac{M}{N_t} \left( 1- \frac\mu{N_t} \right)
	      \Biggl[ 
		\left(
			\frac1{\sqrt\pi\,\sigma} -\frac{\mu}{N_t} \left(1-\frac{\mu}{N_t}\right)
		\right) \times\\\times
		& \quad\left( (\Expect[w])^2 + \Var[w]\right)
		+  \frac{4\Var[w]}{N_t}\left(1-\frac{\mu}{N_t}\right)
	      \Biggr],
	\end{aligned}
    \end{equation}
здесь $\Expect[w]$ и $\Var[w]$ "--- выборочное среднее и дисперсия весов, соответственно.
\end{Theorem}
Доказательство приведено в~\cite{Diplom}.
\begin{Remark}
    Приближенные формулы~(\ref{k_norm}) дают удовлетворительный результат для задачи поиска паттернов, однако,
    точные формулы для значений $\mu_\ast$ и $\sigma_\ast$, можно найти в~\cite{Diplom}. 
\end{Remark}
Теперь для различных пар $\mu$ и $\sigma$ можно вычислить статистику~(\ref{k_stat}),
сравнить ее с $\alpha$-квантилью распределения~(\ref{k_norm}). 
Если гипотеза $H_0$ о <<случайности>> данных будет отвергнута односторонним критерием, 
то считается, что соответствующие P"=Паттерны образуют новый паттерн
с параметрами $\mu$ и $\sigma$. 
Если существует несколько пар $\mu$ и $\sigma$, для которых отвергается гипотеза $H_0$, то для конструирования P"=Паттернов берутся
непересекающиеся\footnote{$[\,\mu'-3\sigma', \mu'+3\sigma']\cap[\,\mu''-3\sigma'', \mu''+3\sigma'']=\varnothing$} параметры
соответствующие максимальным значениям $k$.  

Более подробно о процессе формирования нового P"=Паттерна можно найти в~\cite{Diplom}.

\paragraph{Редукция P"=Паттернов.}
Для удаления~\cite{Diplom} паттернов"=дубликатов и неполных копий анализируется
коэффициент корреляции функций правдоподобия. Пусть $\overrightarrow{L_{\pat,i}}$~---вектор"=столбец
значений функции правдоподобия, отсчитанной от $i$-го события во всех моментах времени наблюдения.
$$
cor\left(\overrightarrow{L_1}, \overrightarrow{L_2}\right) = 
\frac{{\overrightarrow{L_1}}^\top \overrightarrow{L_2}}{ 
\sqrt{ {\overrightarrow{L_1}}^\top \overrightarrow{L_1} }\,\sqrt{ {\overrightarrow{L_2}}^\top \overrightarrow{L_2} } }
\:\in[\,0,1]
$$
~--- коэффициент корреляции между двумя P-Паттернами. 
% Чем он ближе к $1$, тем два паттерна более похожи друг на друга в контексте данных.

% Процедура редукции P"=Паттернов выглядит следующим образом: перебираем все пары $\pat_L, \pat_R$
% из множества найденных паттернов, состоящих из двух и более событий. 
Проверяются все пары паттернов, если все поведенческие акты, присутствующие в паттерне $\pat_L$ 
также присутствуют в $\pat_R$ с учетом порядка, и 
$$
\exists m\colon\: cor\left(\overrightarrow{L_{P_L,1}}, \overrightarrow{L_{P_R,m}} \right) > \nu,
$$
тогда паттерн $\pat_L$ удаляется из множества найденных паттернов.

\paragraph{Алгоритм поиска P-Паттернов} 
% После описания основных шагов конструирования и редукции P-Паттернов, можно описать сам алгоритм
% поиска:
\begin{enumerate}
 \item Инициализировать текущее множество P"=Паттернов событиями(паттерны длины 1).
 \item Для всевозможных пар P"=Паттернов из текущего множества провести процедуру \emph{конструирования}. 
% для которых не было произведено попытки их слития, 
% провести процедуру конструирования паттернов. Сконструированные паттерны, которые встречаются в данных не менее 
% $N_{min}$ раз, добавить в текущее множество. 
 \item Провести процедуру редукции паттернов.
 \item Если текущее множество паттернов изменилось, перейти к п.2.
\end{enumerate}

% Очевидно, что описанный выше метод остановится, так как на каждом шаге будут
% произведены попытки сконструировать паттерны все б\'{о}льшей длины, а одни и те же паттерны
% не проверяются больше одного раза.

Параметры алгоритма и способы их настройки описаны в~\cite{Diplom}.

Сложность предложенного алгоритма "--- $O(n^3)$, 
где $n$ "--- общее количество событий во временном ряде. В~\cite{Diplom}
представлена параллельная реализация данного метода на GPU, что позволило 
применять алгоритм поиска P"=Паттернов на реальных данных.

\section{Эксперименты на реальных данных}
Описанный ниже эксперимент демонстрирует способ применения предложенного метода на реальных поведенческих
данных. Целью эксперимента является анализ того, как влияет отсутствие гиппокампа
на поведение. Гиппокамп~--- один из древнейших отделов головного мозга млекопитающих, его функции 
связывают с механизмами работы памяти
% (консолидация информации из краткосрочной в долгосрочную память)
, обучением, пространственной
навигацией.
% ~\cite[c.~744]{Neuro_exploring_the_brain}

Особи были разделены на 5 групп.
\begin{enumerate*}
 \item Контрольная группа, содержащая разметку поведения здоровых мышей. 12 особей.
 \item Гиппокампальная группа. Гиппокамп этих животных разрушали путем введения в эту структуру лидокаина, растворенного в искусственной спинномозговой жидкости (2 мкл. 4\% 
раствора).
% лидокаин обладает местно-анестезирующим действием, блокирует потенциал-зависимые натриевые каналы, что препятствует генерации импульсов клетками. 
% Используемая доза блокатора вызывала длительную инактивацию клеток структуры, что в дальнейшем приводило к их отмиранию и разрушению части структуры. 
12 особей.
 \item Шумовая группа, с параметрами частоты и продолжительности актов первой \!(контрольной) группы. 12 <<особей>>.
 \item Шумовая группа, с параметрами частоты и продолжительности актов второй \!(гиппокампальной) группы. 12 <<особей>>.
 \item Искусственные данные, содержащие один модельный P"=Паттерн. 7 <<особей>>.
\end{enumerate*}
Поведение каждой особи было представлено временным рядом длины $\sim$ 12 минут, всего было 24 детектируемых поведенческих актов,
более подробно условия эксперимента описаны в~\cite{Diplom}. Требовалось решить задачу классификации особи по поведению.

Далее, данные разбивались на обучение($N_l$ объектов) и контроль($N_c$ объектов). $N_l+N_c=55$. Каждая особь описывалась 
вектором длины $N_l$, $i$-ое значение которого равняется количеству паттернов данной особи, также найденное в
поведении $i$-ой особи из обучающей выборки. 
% Заметим, что таким способом можно закодировать поведение животных используя как P"=Паттерны, 
% так и Т"=Паттерны. Важно, что множество паттернов, экземпляры которых ищутся в поведении 
% особей из обучающей выборки, может быть ограничено по длине и количеству найденных экземпляров, 
% таким образом, сокращается рассмотрение случайных и коротких закономерностей.

% Для сравнения, была приведена наивная классификация особей на основе описания частот и средней продолжительности актов. 
% Каждой особи сопоставлялся вектор из 48-и элементов: первые 24 элемента соответствовали частотам фиксируемых 
% поведенческих актов, вторые 24 элемента соответствовали средним продолжительностям соответствующих 
% поведенческих актов.

\begin{figure}[t]
\label{bd}
\noindent\centering{\includegraphics[width=\linewidth]{bad_data.eps}}
\caption{ Качество~--- средняя доля правильных классификаций.
По горизонтали откладывалось количество объектов в обучении(качество усреднено по ста случайным разбиениям). 
Классификации по P"= и T"= паттернам производилась с помощью SVM.
Классификации на основе частот и длин актов производилась с помощью решающих лесов.  }
\end{figure}

На рис.~2 видно, что предложенный метод поиска P"=Паттернов дает заметно лучшее 
качество классификации, чем метод поиска Т"=Паттернов. 
Наивная классификация на основе описания частот и средней продолжительности актов неприменима в данном эксперименте из-за двух шумовых 
классов.

\paragraph{Характерные паттерны}
присущие определенному классу также могут быть выделены в экспериментальных данных. 
Неформально, паттерн является характерным для заданного класса, 
если он присутствует в поведении многих особей этого класса и редко выявляется в поведении 
животных из других классов. 
% Для формализации данного понятия можно использовать 
% статистическое, энтропийное, или эмпирическое определение информативности
% ~\cite{Voron_Logic}.
Пример характерного для контрольной группы P"=паттерна(в квадратных скобках указаны смещения и дисперсии в секундах между событиями):
\\
<<Вылизывание~гениталий>> $[\,2{,}5; 6{,}2]$~<<Вылизывание~ладоней>> $[\,1{,}2; 7{,}9]$~<<Умывание~головы~с~ушами>> 
$[\,0{,}4; 5{,}7]$~<<Вылизывание~задних~конечностей>> $[\,2{,}6; 7{,}9]$~<<Умывание~носа>>.

\section{Выводы}
Представленный метод решает поставленные перед ним задачи и производит 
качественный поиск закономерностей как в синтетических временн\'{ы}х рядах, так и в реальных
поведенческих данных. В открытом доступе свободная, документированная, параллельная реализация
представленного метода.

Главным преимуществом метода поиска P"=Паттернов является их вариабельность:
если на то есть предпосылки, то P"=Паттерны найденные в поведении одной особи 
будут также найдены в поведении другой особи. Данный факт позволяет описывать 
поведение на основе найденных паттернов, и использовать стандартные алгоритмы 
машинного обучения для решения, например, задач классификации,
кластеризации, или восстановления регрессии.


\begin{thebibliography}{1}

\bibitem{Diplom}
В.В.~Вишневский. «Параллельная реализация метода поиска закономерностей в
последовательностях событий.~--- Дипломная работа. ВМиК МГУ, 2011.

\bibitem{Magnusson}
M.S.~Magnusson. Discovering hidden time patterns in behavior:
T-patterns and their detection.~--- Behavior Research Methods, Instruments, Computers
2000.

\bibitem{Martin_Bateson}
P.~Martin, P.~Bateson. Measuring Behaviour: An Introductory Guide.~--- Cambridge University Press, second edition, 1993.

\bibitem{Stoop}
R.~Stoop, B.~Arthur. Periodic orbit analysis demonstrates genetic constraints, variability, and switching in Drosophila courtship behavior.
~---~Chaos~--~2008~--vol.18/2.

% \bibitem{POrbits}
% P.~Cvitanovic. Periodic orbits as the skeleton of classical and quantum chaos.~
% //~Physica D: Nonlinear Phenomena~--~1991~--~vol.51~--~Issues 1-3~--~Pp. 138--151.

% \bibitem{Shurygin}
% А.М.~Шурыгин. Математические методы прогнозирования.
% ~--- Горячая Линия~--~Телеком, 2009. ISBN   978-5-9912-0062-2.
% 
% \bibitem{Neuro_exploring_the_brain}
% M.F.~Bear, B.W.~Connors, M.A.~Paradiso. Neuroscience: Exploring the Brain.~--- Lippincott Williams \& Wilkins~--~3d editin, 2006.
% 
% \bibitem{Voron_Logic}
% К.В.~Воронцов.
% Лекции по логическим алгоритмам классификации.~--- 2010.

\end{thebibliography}

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
